{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchsummary import summary\n",
    "import os\n",
    "\n",
    "from ray import tune, init\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import streamlit as st\n",
    "import numpy as np"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# noinspection PyTypeChecker\n",
    "class ConvolutionalNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # 3: colors - R G B, 6: output layer size, 5: convolution kernel size\n",
    "        self.pool = nn.MaxPool2d(4, 4)  #4: Pool size, 4: Stride size\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.LazyLinear(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "        self.model_feature_learning = torch.nn.Sequential(\n",
    "            self.conv1,\n",
    "            nn.ReLU(),\n",
    "            self.pool,\n",
    "            self.conv2,\n",
    "            nn.ReLU(),\n",
    "            self.pool\n",
    "        )\n",
    "\n",
    "        self.model_classification = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            self.fc2,\n",
    "            nn.ReLU(),\n",
    "            self.fc3\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # n: number samples in a batch.\n",
    "        # Start with n, 3, 32, 32\n",
    "        x = self.model_feature_learning(x)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.model_classification(x)\n",
    "        return x\n",
    "\n",
    "# List of pre-trained models for fine-tuning\n",
    "models = {\n",
    "    'dense121' : torchvision.models.densenet121(pretrained=True, progress=True),\n",
    "    'tf_effinet_b4': torch.hub.load('rwightman/gen-efficientnet-pytorch', 'tf_efficientnet_b4_ns', pretrained=True),\n",
    "    'effinet_b3': torch.hub.load('rwightman/gen-efficientnet-pytorch', 'efficientnet_b3', pretrained=True)\n",
    "}\n",
    "\n",
    "def data_loader_cnn(_data_dir):\n",
    "    _test_dir = os.path.join(data_dir, 'test')\n",
    "    _train_dir = os.path.join(data_dir, 'train')\n",
    "    _val_dir = os.path.join(data_dir, 'val')\n",
    "\n",
    "    _transform = transforms.Compose(\n",
    "        [transforms.Grayscale(num_output_channels=1),\n",
    "         transforms.ToTensor()])\n",
    "    _dataset_test = datasets.ImageFolder(_test_dir, transform=_transform)\n",
    "    _dataset_train = datasets.ImageFolder(_train_dir, transform=_transform)\n",
    "    _dataset_val = datasets.ImageFolder(_val_dir, transform=_transform)\n",
    "\n",
    "    _loader_test = DataLoader(_dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    _loader_train = DataLoader(_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    _loader_val = DataLoader(_dataset_val, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return _loader_test, _loader_train, _loader_val, _dataset_test, _dataset_train, _dataset_val\n",
    "\n",
    "def data_loader_fine_tune(_data_dir):\n",
    "    _test_dir = os.path.join(_data_dir, 'test')\n",
    "    _train_dir = os.path.join(_data_dir, 'train')\n",
    "    _val_dir = os.path.join(_data_dir, 'val')\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.RandomRotation(degrees=(-20, +20)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    transform_test_val = transforms.Compose(\n",
    "        [transforms.Resize(256),\n",
    "         transforms.CenterCrop(224),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    _dataset_test = datasets.ImageFolder(_test_dir, transform=transform_test_val)\n",
    "    _dataset_train = datasets.ImageFolder(_train_dir, transform=transform)\n",
    "    _dataset_val = datasets.ImageFolder(_val_dir, transform=transform_test_val)\n",
    "    \n",
    "    _loader_test = DataLoader(_dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    _loader_train = DataLoader(_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    _loader_val = DataLoader(_dataset_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return _loader_test, _loader_train, _loader_val, _dataset_test, _dataset_train, _dataset_val\n",
    "\n",
    "# writer = SummaryWriter('runs/mnist')\n",
    "# Use local GPU for CNN models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n",
      "Using cache found in C:\\Users\\Wenda/.cache\\torch\\hub\\rwightman_gen-efficientnet-pytorch_master\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set ML configuration\n",
    "\n",
    "# Configure Streamlit layout\n",
    "# c1, c2 = st.beta_columns((1, 2))\n",
    "# b1 = st.image\n",
    "\n",
    "# Pick image size\n",
    "# image_size = c1.number_input('Choose the size of training image, between 0 - 2000', min_value=0, max_value=2000, step=1, value=500)\n",
    "\n",
    "config = {\n",
    "    \"lr\" : 0.01,\n",
    "    \"momentum\" : 1,\n",
    "}\n",
    "\n",
    "image_size = 300\n",
    "# c1.write(f'Selected {image_size} as image for the model')\n",
    "# Image directory, linked to selected image size\n",
    "data_dir =  os.path.abspath(rf'../Pneumonia_classification_data/reshape_{image_size}')\n",
    "# Pick number of epochs\n",
    "# num_epochs = c1.number_input('Choose the number of epochs for training.', min_value=0, step=1, value=10)\n",
    "# c1.write(f'Selected {num_epochs} as the training epochs')\n",
    "num_epochs = 50\n",
    "# Pick batch size\n",
    "# batch_size = c1.number_input('Choose the number of batch size for training.', min_value=0, step=1, value=4)\n",
    "# c1.write(f'Selected {batch_size} as the training batch size')\n",
    "batch_size = 4\n",
    "# Pick learning rate\n",
    "# rate_learning = c1.number_input('Choose the number of batch size for training.', min_value=0.0, value=0.001)\n",
    "rate_learning = 0.01\n",
    "# Available classes\n",
    "classes = ('Normal', 'Pneumonia')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load and transform datasets\n",
    "# Images are processed from main.py to 300x300 greyscale jpeg format\n",
    "loader_test, loader_train, loader_val, dataset_test, dataset_train, dataset_val = data_loader_cnn(_data_dir=data_dir)\n",
    "\n",
    "# Check loaded data samples\n",
    "images, labels = next(iter(loader_train))\n",
    "# print(images[0])\n",
    "# print(images.shape)\n",
    "# plt.imshow(torchvision.utils.make_grid(images).permute(1,2,0))\n",
    "# print(f'The answer of images are {labels}')\n",
    "lst = [x.item() for x in labels]\n",
    "new_dict = dict((v, k) for k, v in dataset_test.class_to_idx.items())\n",
    "new_lst = [new_dict.get(item) for item in lst]\n",
    "# print(new_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def train(_model, _optimizer, _loader_train):\n",
    "    _model.train()\n",
    "    for batch_dix, (_images, _labels) in enumerate(_loader_train):\n",
    "\n",
    "        _images, _labels = _images.to(device), _labels.to(device)\n",
    "        _optimizer.zero_grad()\n",
    "        _output = _model(_images)\n",
    "        _loss = F.nll_loss(_output, _labels)\n",
    "        _loss.backward()\n",
    "        _optimizer.step()\n",
    "\n",
    "\n",
    "def test(_model, _loader_test):\n",
    "    _model.eval()\n",
    "    _correct = 0\n",
    "    _total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_dix, (_images, _labels) in enumerate(_loader_test):\n",
    "            _images, _labels = _images.to(device), _labels.to(device)\n",
    "            _output = _model(_images)\n",
    "            _, _predicted = torch.max(_output.data, 1)\n",
    "            _total += _labels.size(0)\n",
    "            _correct += (_predicted == _labels).sum().item()\n",
    "        return _correct / _total\n",
    "\n",
    "\n",
    "def train_dense(_config=dict):\n",
    "    _model = ConvolutionalNeuralNet().to(device)\n",
    "    _optimizer = torch.optim.SGD(_model.parameters(), lr=_config['lr'], momentum=_config[\"momentum\"])\n",
    "    for _i in range(num_epochs):\n",
    "        train(_model, _optimizer, loader_train)\n",
    "        _acc = test(_model, loader_test)\n",
    "        tune.report(mean_accuracy=_acc)\n",
    "\n",
    "        torch.save(_model.state_dict(), \"./model_state.pth\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup model, loss, optimizer and total training steps\n",
    "# model = ConvolutionalNeuralNet().to(device)\n",
    "model = models['dense121'].to(device)\n",
    "# print(model)\n",
    "summary(model, input_size=(3, image_size, image_size))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=rate_learning)\n",
    "# TODO: Try different schedulers\n",
    "\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=1285*num_epochs)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1285*2, gamma=0.7)\n",
    "n_total_steps = len(loader_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    \"lr\" : tune.sample_from(lambda spec: 10**(-10 * np.random.rand())),\n",
    "    \"momentum\" : tune.uniform(0.1, 0.9)\n",
    "}\n",
    "\n",
    "analysis = tune.run(train_dense, config=search_space, resources_per_trial={'gpu' : 1})\n",
    "dfs = analysis.trial_dataframes\n",
    "[d.mean_accuracy.plot() for d in dfs.values()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(dfs.values())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 7, 7], expected input[4, 1, 300, 300] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-51-cbed605ebf35>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[1;31m#    caption=new_lst)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 21\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     22\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\densenet.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 216\u001B[1;33m         \u001B[0mfeatures\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    217\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrelu\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minplace\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    218\u001B[0m         \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madaptive_avg_pool2d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mout\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    138\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 139\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    140\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    141\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1051\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1052\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    441\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    442\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 443\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    437\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    438\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[1;32m--> 439\u001B[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[0;32m    440\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[0;32m    441\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[4, 1, 300, 300] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# Start model training\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "\n",
    "# loss_table = c2.line_chart()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(loader_train):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # lst_images = [image[0] for image in images]\n",
    "        # lst = [x.item() for x in labels]\n",
    "        # new_dict = dict((v, k) for k, v in dataset_test.class_to_idx.items())\n",
    "        # new_lst = [new_dict.get(item) for item in lst]\n",
    "        # b1([lst_images],\n",
    "        #    caption=[new_lst])\n",
    "        # b1(torchvision.utils.make_grid(images).permute(1,2,0),\n",
    "        #    caption=new_lst)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + i/ n_total_steps)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.8f}')\n",
    "            # writer.add_scalar('Loss/train', running_loss / 100, epoch * n_total_steps + i)\n",
    "            # writer.add_scalar('Accuracy/train', running_correct / 100, epoch * n_total_steps + i)\n",
    "            # loss_table.add_rows(np.array([loss.item()]))\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 87.33974358974359 %\n",
      "Accuracy of Normal: 69.66 %\n",
      "Accuracy of Pneumonia: 97.95 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(2)]\n",
    "    n_class_samples = [0 for i in range(2)]\n",
    "    for images, labels in loader_test:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predictions == labels).sum().item()\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predictions[i]\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(2):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc:.2f} %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}